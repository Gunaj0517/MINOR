# Driver Monitoring System ğŸš˜ğŸ‘€

An AI-powered driver safety system that integrates **Drowsiness Detection**, **Emotion Recognition**, and **Gesture Control** into a single, real-time pipeline. The project uses **OpenCV**, **MediaPipe**, and **Deep Learning models** to monitor driver state and provide timely alerts to prevent accidents.

---

## ğŸ“Œ Features

* **Drowsiness Detection** â€“ Uses Eye Aspect Ratio (EAR) and Mouth Aspect Ratio (MAR) to detect prolonged eye closure, yawning, and head nodding.
* **Emotion Detection** â€“ Classifies driver emotions (happy, sad, angry, stressed, neutral) using a CNN trained on FER-2013 dataset.
* **Gesture Control** â€“ Enables non-intrusive infotainment control (volume, calls, navigation) using MediaPipe Hand gestures.
* **Real-time Alerts** â€“ Generates audio/visual warnings when unsafe driver states are detected.
* **Cost-Effective** â€“ Works with just a webcam, no additional sensors required.

---

## ğŸ› ï¸ Tech Stack

* **Language:** Python 3
* **Libraries/Frameworks:** OpenCV, MediaPipe, TensorFlow/Keras, NumPy
* **Dataset:** FER-2013 (for emotion detection)
* **IDE:** Jupyter Notebook / VS Code

---

## ğŸ“‚ Project Structure

```
Driver-Monitoring-System/
â”‚â”€â”€ data/                # Datasets (FER-2013, sample videos/images)
â”‚â”€â”€ models/              # Pre-trained/fine-tuned CNN models
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ drowsiness.py    # EAR & MAR based detection
â”‚   â”œâ”€â”€ emotion.py       # Emotion classification using CNN
â”‚   â”œâ”€â”€ gesture.py       # Hand gesture recognition
â”‚   â”œâ”€â”€ main.py          # Integrated pipeline
â”‚â”€â”€ requirements.txt     # Dependencies
â”‚â”€â”€ README.md            # Project documentation
```

---

## ğŸš€ How to Run

1. Clone the repository

   ```bash
   git clone https://github.com/your-username/driver-monitoring-system.git
   cd driver-monitoring-system
   ```
2. Install dependencies

   ```bash
   pip install -r requirements.txt
   ```
3. Run the system

   ```bash
   python src/main.py
   ```

---

## ğŸ“Š Experimental Results

* Drowsiness Detection Accuracy: **91%**
* Emotion Detection Accuracy: **82%**
* Gesture Recognition Accuracy: **95%**
* Average Frame Rate: **15â€“20 FPS (CPU)**

---

## ğŸ”® Future Improvements

* Adaptive preprocessing for extreme lighting conditions
* Dataset expansion for diverse emotions & gestures
* GPU acceleration for higher FPS
* Integration with IoT/automotive hardware for deployment

---

## ğŸ‘¥ Team

Developed by **\[Your Team Name]** â€“ 3rd Year CSE Students

* Member 1
* Member 2
* Member 3
* Member 4

---

## ğŸ“œ License

This project is licensed under the MIT License â€“ feel free to use and modify.

---
